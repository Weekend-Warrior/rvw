% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{vwsetup}
\alias{vwsetup}
\title{Create Vowpal Wabbit model, setup model parameters and data}
\usage{
vwsetup(learning_mode = c("binary", "multiclass", "lda", "factorization",
  "bootstrap", "nn"), algorithm = c("sgd", "bfgs", "ftrl"),
  general_params = list(), optimization_params = list(),
  learning_params = list(), dir = tempdir(), train_data = "",
  test_data = "", model = "mdl.vw", update_model = TRUE, eval = FALSE)
}
\arguments{
\item{learning_mode}{Learning method or reduction:
\itemize{
 \item \code{binary}
 \item \code{multiclass}
 \item \code{lda} - Latent Dirichlet Allocation
 \item \code{factorization} - matrix factorization mode
 \item \code{bootstrap} - bootstrap with K rounds by online importance resampling
 \item \code{ksvm} - online kernel Support Vector Machine
 \item \code{nn} - sigmoidal feedforward network
 \item \code{boosting} - online boosting with weak learners
}}

\item{algorithm}{Optimzation algorithm
\itemize{
 \item \code{sgd} adaptive, normalized, invariant stochastic gradient descent
 \item \code{bfgs}
 \item \code{ftrl}
}}

\item{general_params}{List of parameters:
\itemize{
 \item \code{cache} - Create and use cache files
 \item \code{passes} - Number of Training Passes
 \item \code{bit_precision} - number of bits in the feature table
 \item \code{quadratic} - Create and use quadratic features
 \item \code{cubic} - Create and use cubic features
 \item \code{interactions} - Create feature interactions of any level between namespaces
 \item \code{permutations} - Use permutations instead of combinations for feature interactions of same namespace
 \item \code{holdout_period} - holdout period for test only
 \item \code{early_terminate} - Specify the number of passes tolerated when holdout loss doesn't decrease before early termination
 \item \code{sort_features} - Turn this on to disregard order in which features have been defined. This will lead  to smaller cache sizes
 \item \code{noconstant} - Don't add a constant feature
 \item \code{ngram} - Generate N grams
 \item \code{skips} - Generate skips in N grams
 \item \code{hash} - How to hash the features. Available options: strings, all
 \item \code{affix} - Generate prefixes/suffixes of features
 \item \code{random_weights} - Make initial weights random
 \item \code{sparse_weights} - Use a sparse datastructure for weights
 \item \code{initial_weight} - Set all weights to initial value
}}

\item{optimization_params}{List of parameters:
\itemize{
 \item \code{hessian_on} - Use second derivative in line search
 \item \code{initial_pass_length} - Initial number of examples per pass
 \item \code{l1} - L1 regularization
 \item \code{l2} - L2 regularization
 \item \code{decay_learning_rate} - Set Decay factor for learning_rate between passes
 \item \code{initial_t} - initial t value
 \item \code{power_t} - t power value
 \item \code{learning_rate} - Set initial learning Rate
 \item \code{link} - Convert output predictions using specified link function. Options: identity, logistic, glf1 or poisson.
 \item \code{loss_function} - Specify the loss function to be used
 \item \code{quantile_tau} - Parameter Tau associated with Quantile loss
}
Additional parameters depending on \code{algorithm} choice:
\itemize{
 \item \code{sgd}: 
   \itemize{
     \item \code{adaptive} - Use adaptive, individual learning rates (on by default)
     \item \code{normalized} - Use per feature normalized updates (on by default)
     \item \code{invariant} - Use safe/importance aware updates (on by default)
   }
 \item \code{bfgs}: 
   \itemize{
     \item \code{conjugate_gradient} - Use conjugate gradient based optimization
   }
 \item \code{ftrl}: 
   \itemize{
     \item \code{ftrl_alpha}
     \item \code{ftrl_beta}
   }
}}

\item{learning_params}{List of parametrs associated with learning_mode
\itemize{
 \item \code{binary}: 
   \itemize{
     \item \code{binary} - Reports loss as binary classification with -1,1 labels
   }
 \item \code{multiclass}:
   \itemize{ 
     \item \code{reduction} - csoaa, oaa, ect, wap, csoaa_ldf multiclass learning
     \item \code{num_classes} - Number of classes
   }
 \item \code{lda}:
   \itemize{ 
     \item \code{num_topics} - Number of topics
     \item \code{lda_alpha} - Prior on sparsity of per-document topic weights
     \item \code{lda_rho} - Prior on sparsity of topic distributions
     \item \code{lda_D} - Number of documents
     \item \code{lda_epsilon} - Loop convergence threshold
     \item \code{math_mode} - Math mode: simd, accuracy, fast-approx
     \item \code{minibatch} - Minibatch size
   }
 \item \code{factorization}:
   \itemize{
   \item \code{rank} - rank for matrix factorization
   }
 \item \code{bootstrap}:
   \itemize{
     \item \code{rounds} - number of rounds
     \item \code{bs_type} - the bootstrap mode: 'mean' or 'vote'
   }
 \item \code{nn}:
   \itemize{
     \item \code{hidden} - number of hidden units
     \item \code{inpass} - Train or test sigmoidal feedforward network with input passthrough
     \item \code{multitask} - Share hidden layer across all reduced tasks
     \item \code{dropout} - Train or test sigmoidal feedforward network using dropout.
     \item \code{meanfield} - Train or test sigmoidal feedforward network using mean field.
   }
 \item \code{boosting}:
   \itemize{
     \item \code{num_learners} - number of weak learners
   }
}}

\item{dir}{Working directory, default is tempdir()}

\item{train_data}{Train data file name. File should be in .vw plain text format}

\item{test_data}{Validation data file name. File should be in .vw plain text format}

\item{model}{File name for model weights}

\item{update_model}{Update an existing model, when training with new data. \code{TRUE} by default.}

\item{eval}{Compute model evaluation}
}
\value{
vwmodel list class
}
\description{
Sets up VW model together with parameters and data
}
\examples{
vwsetup(
 dir = tempdir(),
 train_data = "binary_train.vw",
 test_data = "binary_valid.vw",
 model = "pk_mdl.vw",
 general_params = list(cache = TRUE, passes=10),
 optimization_params = list(adaptive=FALSE),
 learning_params = list(binary=TRUE)
)

}
