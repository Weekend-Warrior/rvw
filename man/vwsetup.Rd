% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{vwsetup}
\alias{vwsetup}
\title{Create Vowpal Wabbit model, setup model parameters and data}
\usage{
vwsetup(algorithm = c("sgd", "bfgs", "ftrl", "ksvm"),
  general_params = list(), optimization_params = list(), dir = tempdir(),
  model = "mdl.vw", eval = FALSE, reduction = c("", "binary", "oaa",
  "ect", "csoaa", "wap", "log_multi", "lda", "mf", "lrq", "stage_poly",
  "bootstrap", "autolink", "cb", "cbify", "nn", "topk", "struct_search",
  "boosting"), ...)
}
\arguments{
\item{algorithm}{Optimzation algorithm
\itemize{
 \item \code{sgd} adaptive, normalized, invariant stochastic gradient descent
 \item \code{bfgs}
 \item \code{ftrl}
 \item \code{ksvm}
}}

\item{general_params}{List of parameters:
\itemize{
 \item \code{cache} - Create and use cache files
 \item \code{passes} - Number of Training Passes
 \item \code{bit_precision} - number of bits in the feature table
 \item \code{quadratic} - Create and use quadratic features
 \item \code{cubic} - Create and use cubic features
 \item \code{interactions} - Create feature interactions of any level between namespaces
 \item \code{permutations} - Use permutations instead of combinations for feature interactions of same namespace
 \item \code{holdout_period} - holdout period for test only
 \item \code{early_terminate} - Specify the number of passes tolerated when holdout loss doesn't decrease before early termination
 \item \code{sort_features} - Turn this on to disregard order in which features have been defined. This will lead  to smaller cache sizes
 \item \code{noconstant} - Don't add a constant feature
 \item \code{ngram} - Generate N grams
 \item \code{skips} - Generate skips in N grams
 \item \code{hash} - How to hash the features. Available options: strings, all
 \item \code{affix} - Generate prefixes/suffixes of features
 \item \code{random_weights} - Make initial weights random
 \item \code{sparse_weights} - Use a sparse datastructure for weights
 \item \code{initial_weight} - Set all weights to initial value
}}

\item{optimization_params}{List of parameters:
\itemize{
 \item \code{hessian_on} - Use second derivative in line search
 \item \code{initial_pass_length} - Initial number of examples per pass
 \item \code{l1} - L1 regularization
 \item \code{l2} - L2 regularization
 \item \code{decay_learning_rate} - Set Decay factor for learning_rate between passes
 \item \code{initial_t} - initial t value
 \item \code{power_t} - t power value
 \item \code{learning_rate} - Set initial learning Rate
 \item \code{link} - Convert output predictions using specified link function. Options: identity, logistic, glf1 or poisson.
 \item \code{loss_function} - Specify the loss function to be used
 \item \code{quantile_tau} - Parameter Tau associated with Quantile loss
}
Additional parameters depending on \code{algorithm} choice:
\itemize{
 \item \code{sgd}: 
   \itemize{
     \item \code{adaptive} - Use adaptive, individual learning rates (on by default)
     \item \code{normalized} - Use per feature normalized updates (on by default)
     \item \code{invariant} - Use safe/importance aware updates (on by default)
   }
 \item \code{bfgs}: 
   \itemize{
     \item \code{conjugate_gradient} - Use conjugate gradient based optimization
   }
 \item \code{ftrl}: 
   \itemize{
     \item \code{ftrl_alpha}
     \item \code{ftrl_beta}
   }
}}

\item{dir}{Working directory, default is tempdir()}

\item{model}{File name for model weights}

\item{eval}{Compute model evaluation}

\item{reduction}{Add reduction: 
\itemize{
 \item \code{binary} - Reports loss as binary classification with -1,1 labels
 \item \code{oaa} - One-against-all multiclass learning with  labels
 \item \code{ect} - Error correcting tournament with  labels
 \item \code{csoaa} - One-against-all multiclass learning with  costs
 \item \code{wap} - Weighted all-pairs multiclass learning with  costs
 \item \code{log_multi} - Online (decision) trees for  classes
 \item \code{lda} - Latent Dirichlet Allocation
 \item \code{mf} - Matrix factorization mode
 \item \code{lrq} - Low rank quadratic features
 \item \code{stage_poly} - Stagewise polynomial features
 \item \code{bootstrap} - bootstrap with K rounds by online importance resampling
 \item \code{autolink} - Create link function with polynomial N
 \item \code{cb} - Contextual bandit learning with K costs
 \item \code{cbify} - Convert multiclass on K classes into a contextual bandit problem
 \item \code{nn} - Sigmoidal feedforward network
 \item \code{topk} - Top K recommendation
 \item \code{struct_search} - Search-based structured prediction (SEARN or DAgger)
 \item \code{boosting} - online boosting with weak learners
}}

\item{...}{Options for reduction
\itemize{
 \item \code{oaa} or \code{ect} or \code{log_multi}:
   \itemize{ 
     \item \code{num_classes} - Number of classes
   }
 \item \code{csoaa} or \code{wap}:
   \itemize{ 
     \item \code{num_classes} - Number of classes
     \item \code{csoaa_ldf} or \code{wap_ldf} - \code{singleline} (Default) or \code{multiline} label dependent features
   }
 \item \code{lda}:
   \itemize{ 
     \item \code{num_topics} - Number of topics
     \item \code{lda_alpha} - Prior on sparsity of per-document topic weights
     \item \code{lda_rho} - Prior on sparsity of topic distributions
     \item \code{lda_D} - Number of documents
     \item \code{lda_epsilon} - Loop convergence threshold
     \item \code{math_mode} - Math mode: simd, accuracy, fast-approx
     \item \code{minibatch} - Minibatch size
   }
 \item \code{mf}:
   \itemize{
     \item \code{rank} - rank for matrix factorization
   }
 \item \code{lrq}:
   \itemize{
     \item \code{features} - low rank quadratic features
     \item \code{lrqdropout} - use dropout training for low rank quadratic features
   }
 \item \code{stage_poly}:
   \itemize{
     \item \code{sched_exponent} - exponent controlling quantity of included features
     \item \code{batch_sz} - multiplier on batch size before including more features
     \item \code{batch_sz_no_doubling} - batch_sz does not double
   }
 \item \code{bootstrap}:
   \itemize{
     \item \code{rounds} - number of rounds
     \item \code{bs_type} - the bootstrap mode: 'mean' or 'vote'
   }
 \item \code{autolink}:
   \itemize{
     \item \code{degree} - polynomial degree
   }
 \item \code{cb}:
   \itemize{
     \item \code{costs} - number of costs
   }
 \item \code{cbify}:
   \itemize{
     \item \code{num_classes} - number of classes
   }
 \item \code{nn}:
   \itemize{
     \item \code{hidden} - number of hidden units
     \item \code{inpass} - Train or test sigmoidal feedforward network with input passthrough
     \item \code{multitask} - Share hidden layer across all reduced tasks
     \item \code{dropout} - Train or test sigmoidal feedforward network using dropout.
     \item \code{meanfield} - Train or test sigmoidal feedforward network using mean field.
   }
 \item \code{topk}:
   \itemize{
     \item \code{k} - number of top k recomendations
   }
 \item \code{struct_search}:
   \itemize{
     \item \code{id} - maximum action id or 0 for LDF
   }
 \item \code{boosting}:
   \itemize{
     \item \code{num_learners} - number of weak learners
   }
}}
}
\value{
vwmodel list class
}
\description{
Sets up VW model together with parameters and data
}
\examples{
vwsetup(
 dir = tempdir(),
 model = "pk_mdl.vw",
 general_params = list(cache = TRUE, passes=10),
 optimization_params = list(adaptive=FALSE),
 reduction = "binary"
)

}
